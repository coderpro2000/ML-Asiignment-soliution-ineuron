{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47219cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "n market basket analysis, association rules are used to predict the likelihood of products being purchased together. Association rules count the frequency of items that occur together, seeking to find associations that occur far more often than expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad8e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Apriori algorithm uses frequent itemsets to generate association rules, and it is designed to work on the databases that contain transactions.\n",
    "...\n",
    "Step-4: Finding the association rules for the subsets:\n",
    "Rules\tSupport\tConfidence\n",
    "A ^B â†’ C\t2\tSup{(A ^B) ^C}/sup(A ^B)= 2/4=0.5=50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381a7c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Apriori algorithm uses frequent itemsets to generate association rules. It is based on the concept that a subset of a frequent itemset must also be a frequent itemset. Frequent Itemset is an itemset whose support value is greater than a threshold value(support)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d29c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 1: Choose the number of clusters k. ...\n",
    "Step 2: Select k random points from the data as centroids. ...\n",
    "Step 3: Assign all the points to the closest cluster centroid. ...\n",
    "Step 4: Recompute the centroids of newly formed clusters. ...\n",
    "Step 5: Repeat steps 3 and 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f335688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "The optimal number of clusters can be defined as follow:\n",
    "Compute clustering algorithm (e.g., k-means clustering) for different values of k. ...\n",
    "For each k, calculate the total within-cluster sum of square (wss).\n",
    "Plot the curve of wss according to the number of clusters k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acfc63a",
   "metadata": {},
   "outputs": [],
   "source": [
    " If variables are huge, then K-Means most of the times computationally faster than hierarchical clustering, if we keep k smalls. 2) K-Means produce tighter clusters than hierarchical clustering, especially if the clusters are globular. K-Means Disadvantages : 1) Difficult to predict K-Value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9238b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "Connectivity models: As the name suggests, these models are based on the notion that the data points closer in data space exhibit more similarity to each other than the data points lying farther away. These models can follow two approaches. In the first approach, they start with classifying all data points into separate clusters & then aggregating them as the distance decreases. In the second approach, all data points are classified as a single cluster and then partitioned as the distance increases. Also, the choice of distance function is subjective. These models are very easy to interpret but lacks scalability for handling big datasets. Examples of these models are hierarchical clustering algorithm and its variants.\n",
    "Centroid models: These are iterative clustering algorithms in which the notion of similarity is derived by the closeness of a data point to the centroid of the clusters. K-Means clustering algorithm is a popular algorithm that falls into this category. In these models, the no. of clusters required at the end have to be mentioned beforehand, which makes it important to have prior knowledge of the dataset. These models run iteratively to find the local optima.\n",
    "Distribution models: These clustering models are based on the notion of how probable is it that all data points in the cluster belong to the same distribution (For example: Normal, Gaussian). These models often suffer from overfitting. A popular example of these models is Expectation-maximization algorithm which uses multivariate normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64519df4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63448c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb62e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20886186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90df089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b51490",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
