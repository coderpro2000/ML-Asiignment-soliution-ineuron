{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a447f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "An ensemble learning method involves combining the predictions from multiple contributing models. ... It is common to divide a prediction problem into subproblems. For example, some problems naturally subdivide into independent but related subproblems and a machine learning model can be prepared for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94ed35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A Voting Classifier is a machine learning model that trains on an ensemble of numerous models and predicts an output (class) based on their highest probability of chosen class as the output. ... Voting Classifier supports two types of votings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcce7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bagging, a Parallel ensemble method (stands for Bootstrap Aggregating), is a way to decrease the variance of the prediction model by generating additional data in the training stage. This is produced by random sampling with replacement from the original set. ... These multisets of data are used to train multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e23168",
   "metadata": {},
   "outputs": [],
   "source": [
    "The study of error estimates for bagged classifiers in Breiman [1996b], gives empirical evidence to show that the out-of-bag estimate is as accurate as using a test set of the same size as the training set. Therefore, using the out-of-bag error estimate removes the need for a set aside test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b38b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random Forest chooses the optimum split while Extra Trees chooses it randomly. However, once the split points are selected, the two algorithms choose the best one between all the subset of features. Therefore, Extra Trees adds randomization but still has optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12701ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "If your AdaBoost ensemble underfits the training data, you can try increasing the number of estimators or reducing the regularization hyperparameters of the base estimator. You may also try slightly increasing the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ba1bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "A problem with gradient boosted decision trees is that they are quick to learn and overfit training data. One effective way to slow down learning in the gradient boosting model is to use a learning rate, also called shrinkage (or eta in XGBoost documentation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
