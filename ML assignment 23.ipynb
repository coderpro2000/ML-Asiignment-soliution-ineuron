{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fe844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Advantages of dimensionality reduction\n",
    "It reduces the time and storage space required.\n",
    "The removal of multicollinearity improves the interpretation of the parameters of the machine learning model.\n",
    "It becomes easier to visualize the data when reduced to very low dimensions such as 2D or 3D.\n",
    "Reduce space complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742b4d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "The curse of dimensionality basically means that the error increases with the increase in the number of features. It refers to the fact that algorithms are harder to design in high dimensions and often have a running time exponential in the dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f88508",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dimensionality reduction (compression of information) is reversible in auto-encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2b0766",
   "metadata": {},
   "outputs": [],
   "source": [
    "Depends on dataset. If it is comprised of points that are perfectly aligned, PCA can reduce the dataset down to 1 dimension and preserve 95% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85391eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA should be used mainly for variables which are strongly correlated. If the relationship is weak between variables, PCA does not work well to reduce data. Refer to the correlation matrix to determine. In general, if most of the correlation coefficients are smaller than 0.3, PCA will not help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807a50b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "So far we have learned that PCA is a good choice for dimensionality reduction and visualization for datasets with a large number of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc862f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Indeed, it often make any sense to chain two different dimensionality reduction algorithms. A common example is using PCA to quickly get rid of a large number of useless dimensions, then applying another much slower dimensionality reduction algorithm, such as LLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc3cf4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc0e4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed018e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
