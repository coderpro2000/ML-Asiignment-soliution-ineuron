{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc90397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prior probability shows the likelihood of an outcome in a given dataset. For example, in the mortgage case, P(Y) is the default\n",
    "rate on a home mortgage, which is 2%. P(Y|X) is called the conditional probability, which provides the probability of an \n",
    "outcome given the evidence, that is, when the value of X is known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c362e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Posterior probability is a revised probability that takes into account new available information. For example, let there be two\n",
    "urns, urn A having 5 black balls and 10 red balls and urn B having 10 black balls and 5 red balls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf61107",
   "metadata": {},
   "outputs": [],
   "source": [
    "The likelihood term, P(Y|X) is the probability of getting a result for a given value of the parameters. It is what you label \n",
    "probability. The posterior and prior terms are what you describe as likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deca027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive Bayes is a simple and powerful algorithm for predictive modeling. ... Naive Bayes is called naive because it assumes that\n",
    "each input variable is independent. This is a strong assumption and unrealistic for real data; however, the technique is very \n",
    "effective on a large range of complex problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5a63c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Bayes Optimal Classifier is a probabilistic model that makes the most probable prediction for a new example. ... Bayes \n",
    "Optimal Classifier is a probabilistic model that finds the most probable prediction using the training data and space of\n",
    "hypotheses to make a prediction for a new data instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c5bb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "Features of Bayesian learning methods:\n",
    "– This provides a more flexible approach to learning than algorithms that completely eliminate a hypothesis if it is found to\n",
    "be inconsistent with any single example. – a probability distribution over observed data for each possible hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb2e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Consistent Learners. • A learner L using a hypothesis H and training data D is said to be a consistent learner if it always \n",
    "outputs a hypothesis with zero error on D whenever H contains such a hypothesis. • By definition, a consistent learner must\n",
    "produce a hypothesis in the version space for H given D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba83ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "When assumption of independent predictors holds true, a Naive Bayes classifier performs better as compared to other models.\n",
    "\n",
    "2. Naive Bayes requires a small amount of training data to estimate the test data. So, the training period is less.\n",
    "\n",
    "3. Naive Bayes is also easy to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d13f9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    ". Main imitation of Naive Bayes is the assumption of independent predictors. Naive Bayes implicitly assumes that all the\n",
    "attributes are mutually independent. In real life, it is almost impossible that we get a set of predictors which are completely\n",
    "independent.\n",
    "\n",
    "2. If categorical variable has a category in test data set, which was not observed in training data set, then model will a\n",
    "ssign a 0 (zero) probability and will be unable to make a prediction. This is often known as Zero Frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7907ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Naive Bayes classifier is a simple classifier that classifies based on probabilities of events. It is the applied commonly to text classification. ... Let us consider sentence classification to classify a sentence to either 'question' or 'statement'. In this case, there are two classes (“question” and “statement”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418b1b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive Bayes classifiers work by correlating the use of tokens (typically words, or sometimes other things), with spam and \n",
    "non-spam e-mails and then using Bayes' theorem to calculate a probability that an email is or is notspam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5155f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive Bayes classifier in sentiment analysis\n",
    "Sentiment analysis is a field dedicated to extracting subjective emotions and feelings from text. One common use of sentiment \n",
    "analysis is to figure out if a text expresses negative or positive feelings. Naive Bayes is a popular algorithm for classifying\n",
    "text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
