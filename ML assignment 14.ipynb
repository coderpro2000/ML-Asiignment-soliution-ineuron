{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8850efa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Supervised learning, as the name indicates, has the presence of a supervisor as a teacher. Basically supervised learning is when we teach or train the machine using data that is well labeled. Which means some data is already tagged with the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6524a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "In the hospital sector, offer an example of supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689474b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear regression for regression problems.\n",
    "Random forest for classification and regression problems.\n",
    "Support vector machines for classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0c2ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "A classification algorithm may predict a continuous value, but the continuous value is in the form of a probability for a class label. A regression algorithm may predict a discrete value, but the discrete value in the form of an integer quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb417c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Popular algorithms that can be used for binary classification include:\n",
    "Logistic Regression.\n",
    "k-Nearest Neighbors.\n",
    "Decision Trees.\n",
    "Support Vector Machine.\n",
    "Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6b1b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM or Support Vector Machine is a linear model for classification and regression problems. It can solve linear and non-linear problems and work well for many practical problems. The idea of SVM is simple: The algorithm creates a line or a hyperplane which separates the data into classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e34ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "What is misclassification cost?\n",
    "In cost-sensitive learning instead of each instance being either correctly or incorrectly classified, each class (or instance) is given a misclassification cost. Thus, instead of trying to optimize the accuracy, the problem is then to minimize the total misclassification cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61edf0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These are the points that help us build our SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1acb372",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM algorithms use a set of mathematical functions that are defined as the kernel. The function of kernel is to take data as input and transform it into the required form. ... These functions can be different types. For example linear, nonlinear, polynomial, radial basis function (RBF), and sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f1234",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM works relatively well when there is a clear margin of separation between classes. SVM is more effective in high dimensional spaces. SVM is effective in cases where the number of dimensions is greater than the number of samples. SVM is relatively memory efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3595d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM algorithm is not suitable for large data sets. SVM does not perform very well when the data set has more noise i.e. target classes are overlapping. In cases where the number of features for each data point exceeds the number of training data samples, the SVM will underperform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40130eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "The relatively low accuracy of kNN is caused by several factors. One of them is that every characteristic of the method has the same result on calculating distance. The solution of this problem is to give weight to each data characteristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435069ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "The optimal K value usually found is the square root of N, where N is the total number of samples. Use an error plot or accuracy plot to find the most favorable K value. KNN performs well with multi-label classes, but you must be aware of the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef53e532",
   "metadata": {},
   "outputs": [],
   "source": [
    "Some Advantages of KNN\n",
    "Quick calculation time.\n",
    "Simple algorithm – to interpret.\n",
    "Versatile – useful for regression and classification.\n",
    "High accuracy – you do not need to compare with better-supervised learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf89539",
   "metadata": {},
   "outputs": [],
   "source": [
    "Some Disadvantages of KNN\n",
    "Accuracy depends on the quality of the data.\n",
    "With large data, the prediction stage might be slow.\n",
    "Sensitive to the scale of the data and irrelevant features.\n",
    "Require high memory – need to store all of the training data.\n",
    "Given that it stores all of the training, it can be computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2974bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "A decision tree is a graphical representation of all the possible solutions to a decision based on certain conditions. Tree models where the target variable can take a finite set of values are called classification trees and target variable can take continuous values (numbers) are called regression trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aa612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "A decision tree is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49354ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntropy is an information theory metric that measures the impurity or uncertainty in a group of observations. It determines how a decision tree chooses to split data. The image below gives a better description of the purity of a set. Source. Consider a dataset with N classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aea406",
   "metadata": {},
   "outputs": [],
   "source": [
    "nformation gain is the reduction in entropy or surprise by transforming a dataset and is often used in training decision trees. Information gain is calculated by comparing the entropy of the dataset before and after a transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06489237",
   "metadata": {},
   "outputs": [],
   "source": [
    "The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
